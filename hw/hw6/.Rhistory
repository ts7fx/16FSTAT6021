var(3)
var(c(1,2))
# Adapted from:  https://beckmw.wordpress.com/2013/02/05/collinearity-and-stepwise-vif-selection
library(MASS)
library(clusterGeneration)
library(car)
install.packages("clusterGeneration")
install.packages("car")
set.seed(23847)
# set data dimensions.
num.predictors = 15
num.observations = 200
# get covariance matrix for predictors, then generate predictor matrix
predictor.cov = genPositiveDefMat(num.predictors, covMethod="unifcorrmat")$Sigma
library(MASS)
library(clusterGeneration)
library(car)
predictor.cov = genPositiveDefMat(num.predictors, covMethod="unifcorrmat")$Sigma
predictor.matrix = mvrnorm(num.observations, rep(0, num.predictors), Sigma=predictor.cov)
# generate coefficients
coefficients = runif(num.predictors, -10, 10)
# generate response:  predictors * coefficients + some noise
coefficients
y = predictor.matrix %*% matrix(coefficients) + rnorm(num.observations, sd=20)
View(y)
model.data = as.data.frame(cbind(y, predictor.matrix))
names(model.data) = c("y", paste("x", seq(1:15), sep=""))
View(model.data)
fit = lm(y ~ ., data = model.data)
summary(fit)
View(predictor.cov)
x1.vif.fit = lm(x1 ~ . -y, data = model.data)
summary(x1.vif.fit)
x1.adj.r.squared = summary(x1.vif.fit)$adj.r.squared
x1.vif = 1 / (1 - x1.adj.r.squared)
# calculate all variance inflation factors
vif(fit)
vif(fit)
x1.vif.fit = lm(x1 ~ . -y, data = model.data)
summary(x1.vif.fit)
x1.adj.r.squared = summary(x1.vif.fit)$adj.r.squared
x1.vif = 1 / (1 - x1.adj.r.squared)
x2.vif.fit = lm(x2 ~ . -y, data = model.data)
summary(x2.vif.fit)
x2.adj.r.squared = summary(x2.vif.fit)$adj.r.squared
x2.vif = 1 / (1 - x2.adj.r.squared)
vif(fit)
x2.vif.fit = lm(x2 ~ ., data = model.data)
summary(x2.vif.fit)
x2.adj.r.squared = summary(x2.vif.fit)$adj.r.squared
x2.vif = 1 / (1 - x2.adj.r.squared)
vif(fit)
x2.vif.fit = lm(x2 ~ .-y, data = model.data)
summary(x2.vif.fit)
x2.adj.r.squared = summary(x2.vif.fit)$adj.r.squared
x2.vif = 1 / (1 - x2.adj.r.squared)
vif(fit)
summary(x1.vif.fit)
x2.vif = 1 / (1 - 0.7694)
x1.vif = 1 / (1 - 0.7694)
vif(fit)
?vif
std.norm.pdf = function(x)
{
pdf = 1 / sqrt(2*pi) * exp(-(x^2/2))
return (pdf)
}
est.kde = function(x, sample, bandwidth)
{
kernel.inputs = (sample - x) / bandwidth
kernel.values = sapply(kernel.inputs, std.norm.pdf)
est = 1 / (length(sample) * bandwidth) * sum(kernel.values)
return (est)
}
sample.x = c(rnorm(1000, mean = 0, sd = 1), rnorm(1000, mean = 5, sd = 1))
hist(sample.x, freq = FALSE, breaks = 100)
rug(sample.x)
eval.points = seq(-3, 10, 0.1)
# get and plot KDE with varying bandwidths
kde = sapply(eval.points, est.kde, sample=sample.x, bandwidth=0.01)
lines(x=eval.points, y=kde, type ='o', col = "red")
kde = sapply(eval.points, est.kde, sample=sample.x, bandwidth=0.2)
lines(x=eval.points, y=kde, type ='o', col = "blue")
kde = sapply(eval.points, est.kde, sample=sample.x, bandwidth=0.5)
lines(x=eval.points, y=kde, type ='o', col = "green")
kde = sapply(eval.points, est.kde, sample=sample.x, bandwidth=1)
lines(x=eval.points, y=kde, type ='o', col = "brown")
kde = sapply(eval.points, est.kde, sample=sample.x, bandwidth=2)
lines(x=eval.points, y=kde, type ='o', col = "yellow")
kde = sapply(eval.points, est.kde, sample=sample.x, bandwidth=100)
lines(x=eval.points, y=kde, type ='o', col = "pink")
library(MASS)
library(RColorBrewer)
# get color palette for heat map
palette.function = colorRampPalette(rev(brewer.pal(11,'Spectral')))
heat.colors = palette.function(32)
# generate first bivariate sample
cov1 = matrix(c(10,5,5,10),2,2)
data1 = mvrnorm(n=1000, c(0,0), cov1)
plot(data1, xlab = "X", ylab = "Y")
# generate second bivariate sample
cov2 = matrix(c(10,-5,-5,10),2,2)
data2 = mvrnorm(n=1000, c(0,20), cov2)
plot(data2, xlab = "X", ylab = "Y")
# combine samples
combined.data = rbind(data1, data2)
plot(combined.data, xlab = "X", ylab = "Y")
# get and display bivariate KDE of combined data with different bandwidths
est = kde2d(combined.data[,1], combined.data[,2], h=0.1, n=c(100,100))  # h=0.1
image(est, col = heat.colors, useRaster=TRUE, asp=1)
est = kde2d(combined.data[,1], combined.data[,2], h=1, n=c(100,100))  # h=1
image(est, col = heat.colors, useRaster=TRUE, asp=1)
est = kde2d(combined.data[,1], combined.data[,2], h=10, n=c(100,100))  # h=10
image(est, col = heat.colors, useRaster=TRUE, asp=1)
est = kde2d(combined.data[,1], combined.data[,2], h=100, n=c(100,100))  # h=100
image(est, col = heat.colors, useRaster=TRUE, asp=1)
# what happens if the first sample is larger than the second?
data2 = mvrnorm(n=500, c(0,20), cov2)
combined.data = rbind(data1, data2)
est = kde2d(combined.data[,1], combined.data[,2], n=c(100,100))
image(est, col = heat.colors, useRaster=TRUE, asp=1)
# Adapted from:  https://beckmw.wordpress.com/2013/02/05/collinearity-and-stepwise-vif-selection
library(MASS)
library(clusterGeneration)
library(car)
# for explanatory purposes, set the seed to an arbitrary value in order to get consistent runs.
set.seed(23847)
# set data dimensions.
num.predictors = 15
num.observations = 200
# get covariance matrix for predictors, then generate predictor matrix
predictor.cov = genPositiveDefMat(num.predictors, covMethod="unifcorrmat")$Sigma
predictor.matrix = mvrnorm(num.observations, rep(0, num.predictors), Sigma=predictor.cov)
# generate coefficients
coefficients = runif(num.predictors, -10, 10)
# generate response:  predictors * coefficients + some noise
y = predictor.matrix %*% matrix(coefficients) + rnorm(num.observations, sd=20)
# assemble model data
model.data = as.data.frame(cbind(y, predictor.matrix))
names(model.data) = c("y", paste("x", seq(1:15), sep=""))
# build linear model. all variables should be significantly correlated with the response. are they?
fit = lm(y ~ ., data = model.data)
summary(fit)
# calculate the variance inflation factor (VIF) for the first variable.
x1.vif.fit = lm(x1 ~ . -y, data = model.data)
summary(x1.vif.fit)
x1.adj.r.squared = summary(x1.vif.fit)$adj.r.squared
x1.vif = 1 / (1 - x1.adj.r.squared)
x2.vif.fit = lm(x2 ~ .-y, data = model.data)
summary(x2.vif.fit)
x2.adj.r.squared = summary(x2.vif.fit)$adj.r.squared
x2.vif = 1 / (1 - x2.adj.r.squared)
# calculate all variance inflation factors
vif(fit)
# remove predictor with highest VIF and repeat.
fit = lm(y ~ . -x11, data = model.data)
setwd("~/Documents/16FSTAT6021/hw/hw6")
library(gdata)
library(car)
library(glmnet)
data.9.19 <- na.omit(read.xls("data-table-B3.xls"))
x.9.19 <- model.matrix(y~.,data.9.19)[,-1]
y.9.19 <-data.9.19$y
# fit ridge regression model to the data
ridge.9.19 <- glmnet(x.9.19, y.9.19, alpha=0)
# plot the ridge trace plot
plot(ridge.9.19,xvar="lambda",label=TRUE)
log(6.5)
## Tianye Song, ts7fx
## STAT 6021, HW6
## 10/9/2016
setwd("~/Documents/16FSTAT6021/hw/hw6")
library(gdata)
library(car)
library(glmnet)
# 9.7
data.9.7 <- read.xls("data-table-B3.xls")
# correlation matrix
cor(data.9.7[,2:12])
kappa(lm.9.7)
# condition number k = 114075.5 exceeds 1000, which indicates that there is at least one strong near-linear dependence in the data set.
# VIF
lm.9.7 <- lm(y~., data=data.9.7)
vif(lm.9.7)
data.9.13 <- read.xls("data-table-B18.xls")
lm.9.13 <- lm(y.~., data=data.9.13)
kappa(lm.9.13)
# condition number is 416182.3, which indicates at least one strong near-linear dependence in the data
vif(lm.9.13)
#       x1         x2         x3         x4         x5         x6         x7         x8
# 1.000000   1.900541 168.467420  43.103776  60.791320 275.472571 185.707184  44.363364
# multiple terms in the VIF are large. Thus multicollinearity is present.
# 9.14
data.9.14 <- read.xls("data-table-B19.xls")
lm.9.14 <- lm(y~., data=data.9.14)
summary(lm.9.14)
data.9.19 <- na.omit(read.xls("data-table-B3.xls"))
x.9.19 <- model.matrix(y~.,data.9.19)[,-1]
y.9.19 <-data.9.19$y
# fit ridge regression model to the data
ridge.9.19 <- glmnet(x.9.19, y.9.19, alpha=0)
# plot the ridge trace plot
plot(ridge.9.19,xvar="lambda",label=TRUE)
# from the ridge trace plot, roughly we can see the direction of changes in coefficients stop around log(lambda) = 3.
# thus, take lambda value of exp(3), which is 20.1
ridge.9.19b <- glmnet(x.9.19, y.9.19, alpha=0, lambda = 20.1)
coef(ridge.9.19b)
ridge.y<- predict.glmnet(ridge.9.19b, x.9.19)
# residual sum of squares for ridge regression
sum((y.9.19-ridge.y)^2)
data.10.14 <- read.xls("data-table-B11.xls")
# modify region information by adding two indicator variables
# region  r1 r2
#      1   0  0
#      2   0  1
#      3   1  0
data.10.14$r1 = as.numeric(data.10.14$Region == 3)
data.10.14$r2 = as.numeric(data.10.14$Region == 2)
# apply the exhaustive all subsets approach
bestmod.10.14 <- regsubsets(Quality~.-Region, data=data.10.14, nbest=5)
# look at CP values for subset selection
summary(bestmod.10.14)
summary(bestmod.10.14)$cp
# number of regressors                                  subset selected    cp
lm.10.2 <- lm(y~x1+x2+x4+x7+x8+x9, data=data.10.1)
library(leaps)
# implement exhaustive subset selection strategy
bestmod <- regsubsets(y~x1+x2+x4+x7+x8+x9, data=data.10.1, nbest=5)
summary(bestmod)
summary(bestmod)$rss
summary(bestmod)$adjr2
summary(bestmod)$cp
library(leaps)
data.10.14 <- read.xls("data-table-B11.xls")
# modify region information by adding two indicator variables
# region  r1 r2
#      1   0  0
#      2   0  1
#      3   1  0
data.10.14$r1 = as.numeric(data.10.14$Region == 3)
data.10.14$r2 = as.numeric(data.10.14$Region == 2)
# apply the exhaustive all subsets approach
bestmod.10.14 <- regsubsets(Quality~.-Region, data=data.10.14, nbest=5)
# look at CP values for subset selection
summary(bestmod.10.14)
summary(bestmod.10.14)$cp
lm.10.14.a <- lm(Quality ~ Flavor+r2+r1, data= data.10.14)
# second choice: Quality ~ Flavor+Oakiness+r2+r1
lm.10.14.b <- lm(Quality ~ Flavor+Oakiness+r2+r1, data= data.10.14)
summary(lm.10.14.b)
plot(lm.10.14.a$residuals)
plot(lm.10.14.b$residuals)
plot(ridge.9.19,xvar="lambda",label=TRUE)
seq(0,1,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
lambdas<-seq(0,1,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
data.9.19 <- na.omit(read.xls("data-table-B3.xls"))
View(data.9.19)
x.9.19 <- model.matrix(y~.,data.9.19)[,-1]
y.9.19 <-data.9.19$y
View(x.9.19)
plot(ridge.9.19,xvar="lambda",label=TRUE)
lambdas<-seq(-1,1,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
lambdas<-seq(0,1,0.01)
lambdas<-seq(0,2,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,3,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,4,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,6,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,100,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,20,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,1000,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,200,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,200,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
lambdas<-seq(0,1,0.01)
ridge.9.19tt <- glmnet(x.9.19, y.9.19, alpha=0,lambda = lambdas)
# plot the ridge trace plot
plot(ridge.9.19tt,xvar="lambda",label=TRUE)
